#!/usr/bin/env python3
"""
Kyutai TTS 1.6B Service for Pipecat v2

Extends Pipecat's TTSService properly:
- Implements run_tts() abstract method
- Yields TTSAudioRawFrame with 24kHz int16 PCM
- Base class handles text aggregation, LLMFullResponseEndFrame flushing, etc.
- BaseOutputTransport handles resampling if needed

Uses the moshi library to load Kyutai TTS 1.6B from HuggingFace cache.
Benchmarked at 5.28x realtime with n_q=8 on RTX 5070 Ti.
"""

import asyncio
import gc
import os
import queue
from typing import AsyncGenerator, Optional

import numpy as np
import torch
from loguru import logger

from pipecat.frames.frames import (
    Frame,
    TTSAudioRawFrame,
    TTSStartedFrame,
    TTSStoppedFrame,
)
from pipecat.services.tts_service import TTSService

# =============================================================================
# Kyutai TTS constants
# =============================================================================

KYUTAI_SAMPLE_RATE = 24000
KYUTAI_MODEL_REPO = "kyutai/tts-1.6b-en_fr"

# Voice reference clip for cloning. Longer clips = better quality.
# See voice_samples/ and sample_voices_kyutai.py for alternatives.
KYUTAI_VOICE = "expresso/ex03-ex01_awe_001_channel1_1323s.wav"

# Performance tuning
KYUTAI_N_Q = 8       # Codec quantization steps — lower = faster (8 → 5.28x RT, 32 → 1.74x)
KYUTAI_TEMP = 0.6    # Generation temperature
KYUTAI_CFG_COEF = 2.0  # Classifier-free guidance strength

# Audio chunking for streaming playback (~100ms chunks)
TTS_CHUNK_MS = 100

# Streaming TTS: batch N codec frames before yielding to avoid excessive tiny frames.
# Each codec frame is ~12.5ms at 24kHz (300 samples). 4 frames ≈ 50ms per yield.
STREAMING_BATCH_FRAMES = 4

# Whether to use streaming TTS (push frames as they're generated vs. collect all first)
STREAMING_TTS_ENABLED = os.getenv("VOICE_STREAMING_TTS", "true").lower() in ("true", "1", "yes")


class KyutaiTTSService(TTSService):
    """Kyutai TTS 1.6B service for Pipecat.

    Loads from HuggingFace cache. Outputs 24kHz mono int16 PCM.
    """

    def __init__(
        self,
        *,
        model_repo: str = KYUTAI_MODEL_REPO,
        voice: str = KYUTAI_VOICE,
        n_q: int = KYUTAI_N_Q,
        temp: float = KYUTAI_TEMP,
        cfg_coef: float = KYUTAI_CFG_COEF,
        device: str = "cuda",
        **kwargs,
    ):
        super().__init__(sample_rate=KYUTAI_SAMPLE_RATE, **kwargs)

        self._model_repo = model_repo
        self._voice = voice
        self._n_q = n_q
        self._temp = temp
        self._cfg_coef = cfg_coef
        self._device_str = device

        self._tts_model: Optional[object] = None  # moshi.models.tts.TTSModel
        self._voice_path: Optional[object] = None  # pathlib.Path
        self._condition_attributes: Optional[object] = None
        self._loaded: bool = False

    async def _ensure_loaded(self) -> None:
        """Load the model on first use (~5s, ~4GB VRAM)."""
        if self._loaded:
            return

        logger.info(f"Loading Kyutai TTS from {self._model_repo} (n_q={self._n_q})...")

        def _load():
            from moshi.models.loaders import CheckpointInfo
            from moshi.models.tts import TTSModel

            device = torch.device(self._device_str)
            checkpoint_info = CheckpointInfo.from_hf_repo(self._model_repo)
            tts_model = TTSModel.from_checkpoint_info(
                checkpoint_info, n_q=self._n_q, temp=self._temp, device=device
            )
            voice_path = tts_model.get_voice_path(self._voice)
            condition_attributes = tts_model.make_condition_attributes(
                [voice_path], cfg_coef=self._cfg_coef
            )
            return tts_model, voice_path, condition_attributes

        try:
            self._tts_model, self._voice_path, self._condition_attributes = (
                await asyncio.get_event_loop().run_in_executor(None, _load)
            )
            self._loaded = True
            vram = torch.cuda.memory_allocated() / 1e9
            logger.info(f"Kyutai TTS loaded. VRAM: {vram:.2f} GB")
        except Exception:
            # Clean up partial state on failure
            self._tts_model = None
            self._voice_path = None
            self._condition_attributes = None
            gc.collect()
            torch.cuda.empty_cache()
            raise

    async def run_tts(self, text: str) -> AsyncGenerator[Frame, None]:
        """Generate speech from text, yielding TTSAudioRawFrame chunks.

        This is the abstract method required by TTSService. The base class
        calls this via process_generator() after text aggregation.

        When STREAMING_TTS_ENABLED is True (default), audio chunks are yielded
        AS they're generated by the codec, cutting time-to-first-audio by
        ~300-700ms. Falls back to batch mode if streaming is disabled.
        """
        if not text.strip():
            return

        await self._ensure_loaded()

        logger.debug(f"TTS generating: {text[:60]}...")

        yield TTSStartedFrame()

        try:
            if STREAMING_TTS_ENABLED:
                async for frame in self._run_tts_streaming(text):
                    yield frame
            else:
                async for frame in self._run_tts_batch(text):
                    yield frame
        except Exception as e:
            logger.error(f"TTS generation error: {e}")
        finally:
            yield TTSStoppedFrame()

    async def _run_tts_streaming(self, text: str) -> AsyncGenerator[Frame, None]:
        """Streaming TTS: yield audio frames as they're generated.

        Uses a thread-safe queue to pass decoded PCM chunks from the
        executor thread (where Kyutai generates) to this async generator.
        Sentinel value None signals generation complete.
        """
        audio_queue: queue.Queue = queue.Queue()
        loop = asyncio.get_event_loop()

        # Run generation in executor — it will push chunks to audio_queue
        gen_future = loop.run_in_executor(
            None, self._generate_sync_streaming, text, audio_queue
        )

        frames_yielded = 0
        try:
            while True:
                # Poll queue with small timeout to stay async-friendly
                try:
                    chunk = await asyncio.wait_for(
                        loop.run_in_executor(None, audio_queue.get, True, 0.5),
                        timeout=1.0,
                    )
                except (asyncio.TimeoutError, queue.Empty):
                    # Check if generation is done (future completed with error)
                    if gen_future.done():
                        # Drain any remaining items
                        while not audio_queue.empty():
                            chunk = audio_queue.get_nowait()
                            if chunk is None:
                                break
                            yield TTSAudioRawFrame(
                                audio=chunk,
                                sample_rate=KYUTAI_SAMPLE_RATE,
                                num_channels=1,
                            )
                            frames_yielded += 1
                        break
                    continue

                if chunk is None:
                    # Sentinel: generation complete
                    break

                yield TTSAudioRawFrame(
                    audio=chunk,
                    sample_rate=KYUTAI_SAMPLE_RATE,
                    num_channels=1,
                )
                frames_yielded += 1
        finally:
            # Ensure the executor task completes
            try:
                await gen_future
            except Exception as e:
                logger.error(f"TTS streaming generation error: {e}")

        if frames_yielded == 0:
            logger.warning("TTS streaming generated no audio frames")
        else:
            logger.debug(f"TTS streaming yielded {frames_yielded} frames")

    async def _run_tts_batch(self, text: str) -> AsyncGenerator[Frame, None]:
        """Batch TTS: generate all audio, then yield in chunks (original behavior)."""
        audio_int16 = await asyncio.get_event_loop().run_in_executor(
            None, self._generate_sync_batch, text
        )

        if audio_int16 is not None and len(audio_int16) > 0:
            chunk_samples = KYUTAI_SAMPLE_RATE * TTS_CHUNK_MS // 1000
            chunk_bytes = chunk_samples * 2  # int16 = 2 bytes per sample

            audio_bytes = audio_int16.tobytes()
            for i in range(0, len(audio_bytes), chunk_bytes):
                chunk = audio_bytes[i : i + chunk_bytes]
                if chunk:
                    yield TTSAudioRawFrame(
                        audio=chunk,
                        sample_rate=KYUTAI_SAMPLE_RATE,
                        num_channels=1,
                    )
        else:
            logger.warning("TTS generated empty audio")

    def _generate_sync_streaming(self, text: str, audio_queue: queue.Queue) -> None:
        """Synchronous TTS generation with streaming output via queue.

        Pushes int16 PCM byte chunks to the queue as they're decoded.
        Sends None sentinel when generation is complete.
        """
        try:
            entries = self._tts_model.prepare_script([text], padding_between=1)

            batch_buffer: list[np.ndarray] = []
            frames_in_batch = 0

            def _on_frame(frame):
                nonlocal frames_in_batch
                if (frame != -1).all():
                    pcm = self._tts_model.mimi.decode(frame[:, 1:, :]).cpu().numpy()
                    pcm_clipped = np.clip(pcm[0, 0], -1, 1)
                    batch_buffer.append(pcm_clipped)
                    frames_in_batch += 1

                    if frames_in_batch >= STREAMING_BATCH_FRAMES:
                        # Flush batch to queue
                        audio_float = np.concatenate(batch_buffer, axis=-1)
                        audio_int16 = (audio_float * 32767).astype(np.int16)
                        audio_queue.put(audio_int16.tobytes())
                        batch_buffer.clear()
                        frames_in_batch = 0

            with self._tts_model.mimi.streaming(1):
                self._tts_model.generate(
                    [entries], [self._condition_attributes], on_frame=_on_frame
                )

            # Flush any remaining frames in the batch
            if batch_buffer:
                audio_float = np.concatenate(batch_buffer, axis=-1)
                audio_int16 = (audio_float * 32767).astype(np.int16)
                audio_queue.put(audio_int16.tobytes())

        except Exception as e:
            logger.error(f"Kyutai TTS streaming generation error: {e}")
        finally:
            # Signal completion
            audio_queue.put(None)

    def _generate_sync_batch(self, text: str) -> Optional[np.ndarray]:
        """Synchronous TTS generation (batch mode). Runs in executor thread.

        Returns int16 numpy array of audio samples at 24kHz.
        """
        try:
            entries = self._tts_model.prepare_script([text], padding_between=1)

            pcms: list[np.ndarray] = []

            def _on_frame(frame):
                if (frame != -1).all():
                    pcm = self._tts_model.mimi.decode(frame[:, 1:, :]).cpu().numpy()
                    pcms.append(np.clip(pcm[0, 0], -1, 1))

            with self._tts_model.mimi.streaming(1):
                self._tts_model.generate(
                    [entries], [self._condition_attributes], on_frame=_on_frame
                )

            if pcms:
                audio_float = np.concatenate(pcms, axis=-1)
                audio_int16 = (audio_float * 32767).astype(np.int16)
                return audio_int16
            return None

        except Exception as e:
            logger.error(f"Kyutai TTS batch generation error: {e}")
            return None

    async def cleanup(self) -> None:
        """Release GPU memory when the service is shut down."""
        if self._tts_model is not None:
            logger.info("Unloading Kyutai TTS model from VRAM")
            del self._tts_model
            self._tts_model = None
            self._voice_path = None
            self._condition_attributes = None
            self._loaded = False
            gc.collect()
            torch.cuda.empty_cache()

    async def cancel(self, frame: Frame) -> None:
        """Handle cancel frame."""
        await super().cancel(frame)

    def can_generate_metrics(self) -> bool:
        return True
